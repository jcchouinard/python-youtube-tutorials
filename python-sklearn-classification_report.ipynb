{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn Classification_report in Python (with Scikit-Learn)\n",
    "\n",
    "By: jcchouinard.com\n",
    "\n",
    "-----\n",
    "\n",
    "## What is a Classification Report\n",
    "\n",
    "A Classification Report is used to compute the accuracy of a classification model based on the values from the confusion matrix. \n",
    "\n",
    "### Sklearn `classification_report()`\n",
    "\n",
    "In Scikit-learn, `classification_report()` function from `sklearn.metrics` module is used to return a classification report.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9298245614035088"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "X, y = load_breast_cancer(return_X_y=True)\n",
    "\n",
    "# split taining and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, stratify=y)\n",
    "\n",
    "# train the model\n",
    "knn = KNeighborsClassifier(n_neighbors=8)\n",
    "knn.fit(X_train, y_train)\n",
    "y_pred = knn.predict(X_test)\n",
    "\n",
    "# compute accuracy of the model\n",
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90        64\n",
      "           1       0.94      0.95      0.94       107\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.93      0.92      0.92       171\n",
      "weighted avg       0.93      0.93      0.93       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Primer on Confusion Matrix\n",
    "\n",
    "The confusion matrix returns metrics used to make calculations in the classification report.\n",
    "\n",
    "- `True positives (TP)`: predicted true and it is true. Predicted that someone `is sick` and the person `is sick`.\n",
    "- `False positives (FP)`: predicted false and it is false. Predicted that someone `is not sick` and the person `is not sick`.\n",
    "- `True negatives (TN)`: predicted True and it is false. Predicted that someone `is sick` and the person `is not sick`.\n",
    "- `False negatives (FN)`: predicted false and it is true. Predicted that someone `is not sick` and the person `is sick`.\n",
    "\n",
    "![](https://www.jcchouinard.com/wp-content/uploads/2023/09/confusion-matrix.jpg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics in Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90        64\n",
      "           1       0.94      0.95      0.94       107\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.93      0.92      0.92       171\n",
      "weighted avg       0.93      0.93      0.93       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Columns of classification_report\n",
    "- Precision: When we predict True, how often is it true? Higher = Better\n",
    "- Recall (sensitivity): What percentage of the True values did we predict? Higher = Better\n",
    "- F1-Score Combines precision and recall. Useful when opposite scores in precision and recall. Higher = Better\n",
    "- support: number of occurrences of each class in your y_test\n",
    "\n",
    "### Rows of classification_report\n",
    "- 0, 1: Classes (Benign, Malignant)\n",
    "- Accuracy: How often are we predicting the right outcome? Higher = Better\n",
    "- macro avg: Average of metrics across all classes, treating each class equally. (precision_a + precision_b) / 2\n",
    "- weighted avg: Class metrics weighted by class size. (precision_a * support_a) + (precision_b * support_b) / total_support\n",
    "\n",
    "\n",
    "### Recap\n",
    "\n",
    "| Metric                | What it is                                             | Sklearnâ€™s Metric Method                        |\n",
    "|-----------------------|--------------------------------------------------------|------------------------------------------------|\n",
    "| Accuracy              | (true positive + true negative) / total predictions    | `metrics.accuracy_score(true, pred)`           |\n",
    "| Precision             | true positive / (true positive + false positive)       | `metrics.precision_score(true, pred)`          |\n",
    "| Recall (sensitivity)  | true positive / (true positive + false negative)       | `metrics.recall_score(true, pred)`             |\n",
    "| F1-Score              | 2 * (precision * recall) / (precision + recall)        | `metrics.f1_score(true, pred)`                 |\n",
    "| Specificity           | true negative / (true negative + false positive)       | `metrics.recall_score(true, pred, pos_label=0)`|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example with Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 57,   7],\n",
       "       [  5, 102]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    " \n",
    "cm = confusion_matrix(y_test,y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 7 5 102\n"
     ]
    }
   ],
   "source": [
    "tn = cm[0][0] # predicted IS SICK and person IS NOT SICK \n",
    "fp = cm[0][1] # predicted IS NOT SICK and person IS NOT SICK\n",
    "fn = cm[1][0] # predicted IS NOT SICK and person IS SICK \n",
    "tp = cm[1][1] # predicted IS SICK and person IS SICK\n",
    "print(tn, fp, fn, tp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute Metrics from Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.93\n",
      "precision: 0.94\n",
      "recall: 0.95\n",
      "f1_score: 0.94\n"
     ]
    }
   ],
   "source": [
    "accuracy = (tp + tn) / (tp + fp + fn + tn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "print(f'accuracy: {accuracy:.2F}')\n",
    "print(f'precision: {precision:.2F}')\n",
    "print(f'recall: {recall:.2F}')\n",
    "print(f'f1_score: {f1_score:.2F}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90        64\n",
      "           1       0.94      0.95      0.94       107\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.93      0.92      0.92       171\n",
      "weighted avg       0.93      0.93      0.93       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Support class 0: 64\n",
      "Support class 1: 107\n",
      "Support all classes: 171\n"
     ]
    }
   ],
   "source": [
    "print('Support class 0:', tn + fp)\n",
    "print('Support class 1:',tp + fn)\n",
    "print('Support all classes:', tp + fp + fn + tn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpret the Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.92      0.89      0.90        64\n",
      "           1       0.94      0.95      0.94       107\n",
      "\n",
      "    accuracy                           0.93       171\n",
      "   macro avg       0.93      0.92      0.92       171\n",
      "weighted avg       0.93      0.93      0.93       171\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Accuracy: Highlight that 93% accuracy means the model is right 93% of the time.\n",
    "- Precision vs. Recall (Focus on Class 1 - Malignant): in the critical case where we want to correctly predict Cancer, we want precision (correct positive predictions) and recall (all actual positive)\n",
    "    - precision (correct positive predictions). When we predict Cancer, how often is it really Cancer? 94% of the time. High precision: Not many Benign cancers were predicted as Malignant\n",
    "    - recall (capturing all actual positives) is 95%. What percentage of the real cancers did we manage to predict? 95%. High recall: Predicted most Cancers.\n",
    "    \n",
    "This shows how well the model performs in identifying cancer cases, which is vital for real-world applications.\n",
    "\n",
    "- True Positive (TP)\tCorrect positive identification.\n",
    "- True Negative (TN)\tCorrect negative identification.\n",
    "- False Positive (FP)\tIncorrect positive identification.\n",
    "- False Negative (FN)\tIncorrect negative identification.\n",
    "\n",
    "Detailed report\n",
    "- Precision (Class 0 - benign): Out of all the times the model predicted \"benign,\" it was right 92% of the time.\n",
    "- Recall (Class 0 - benign): Out of all actual \"benign\" cases, the model correctly identified 89%.\n",
    "- F1-score (Class 0 - benign): Combines precision and recall, showing that the model is 90% effective at predicting \"benign\" cases.\n",
    "- Precision (Class 1 - malignant): Out of all the times the model predicted \"malignant,\" it was right 94% of the time.\n",
    "\n",
    "Overall:\n",
    "\n",
    "- Accuracy: The model got the predictions right 93% of the time.\n",
    "- Macro Average: Looks at how well the model does for both classes equally, without considering how many cases there are for each class.\n",
    "- Weighted Average: Takes into account that there may be more cases in one class than the other, giving a balanced view of performance across all instances."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Help me and subscribe to this channel.\n",
    "\n",
    "Stay tuned for my upcoming Python for SEO course.\n",
    "\n",
    "### [jcchouinard.com](https://www.jcchouinard.com/)\n",
    "### [youtube.com/@jcchouinard](https://www.youtube.com/@jcchouinard)\n",
    "### [twitter.com/ChouinardJC](https://twitter.com/ChouinardJC)\n",
    "### [linkedin.com/in/jeanchristophechouinard](https://www.linkedin.com/in/jeanchristophechouinard)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
